{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb0b8075",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m spark=\u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFifa21_DataCleaning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\pc1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.master(\"local[*]\").appName(\"Fifa21_DataCleaning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_df_v2 = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .load(\"D:\\Grapple\\PySpark\\DataSets\\Fifa21\\fifa raw data v2.csv\")\n",
    "display(fifa_df_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12298949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the column Name\n",
    "\n",
    "fifa_df_v2=fifa_df_v2.withColumnRenamed(\"â†“OVA\",\"OVA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6303f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modyfying the column Club Removing the \"\\n\" from the column using regexp_replace\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "fifa_df_v2=fifa_df_v2.withColumn(\"Club\",regexp_replace(col(\"Club\"),\"\\n\",\"\"))\n",
    "fifa_df_v2.select(\"Club\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Replacing the column Height from String to integer\n",
    "def replaceheight(x):\n",
    "    x=x.strip()                             # removing the space using strip()\n",
    "    if x is None:   \n",
    "        return None\n",
    "    if \"'\" in x:            \n",
    "        feet, foot = x.split(\"'\")           # assigning the value to feet and foot by spliting based on (')\n",
    "        if \"\\\"\" in foot:\n",
    "            foot=foot.replace(\"\\\"\",\"\")      # replacing the \" with \"\"\n",
    "        if foot==\"\":\n",
    "            foot=0                          # if foot is empty then foot is 0\n",
    "        \n",
    "        return int(int(feet) * 30.48 + int(foot) * 2.54)    #converting  the value to cm\n",
    "    elif \"cm\" in x:\n",
    "        return int(x.replace(\"cm\",\"\"))      # replacing the cm with \"\" an tycasting to int \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# using userdefine function to replace the value\n",
    "returnntype=udf(replaceheight,StringType())\n",
    "# using with colummn to replace the value and casting the value to int\n",
    "fifa_df_v2=fifa_df_v2.withColumn(\"Height\", returnntype(col(\"Height\"))).withColumn(\"Height\",col(\"Height\").cast(\"int\"))\n",
    "\n",
    "# Display how many types of data in the height column using regexp_replace, group by and count\n",
    "display(fifa_df_v2.withColumn(\"Height\",regexp_replace(col(\"Height\"),\"[0-9]\",\"\")).groupBy(\"Height\").count().orderBy(\"count\"))\n",
    "display(fifa_df_v2.select(\"Height\").dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Method 2\n",
    "\n",
    "from pyspark.sql.functions import col, when, split, regexp_replace\n",
    "\n",
    "fifa_df_v2 = fifa_df_v2.withColumn(\n",
    "    \"Height\",\n",
    "    when(col(\"Height\").like(\"%'%\"),\n",
    "         # Feet â†’ split before '\n",
    "         split(col(\"Height\"), \"'\").getItem(0).cast(\"int\") * 30.48 +\n",
    "         # Inches â†’ split after ' then remove all non-digits like \" or ''\n",
    "         regexp_replace(split(col(\"Height\"), \"'\").getItem(1), r'[^0-9]', \"\").cast(\"int\") * 2.54\n",
    "    )\n",
    "    .when(col(\"Height\").like(\"%cm%\"),\n",
    "          regexp_replace(col(\"Height\"), \"cm\", \"\").cast(\"int\"))\n",
    "    .otherwise(None)\n",
    ").withColumn(\"Height\", col(\"Height\").cast(\"int\"))\n",
    "\n",
    "#fifa_df_v2.select(\"Height\", \"Height_cm\").show(20, False)\n",
    "\n",
    "display(fifa_df_v2.withColumn(\"Height\",regexp_replace(col(\"Height\"),\"[0-9]\",\"\")).groupBy(\"Height\").count().orderBy(\"count\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the lbs to kg from weight column  and converting the weight column to int\n",
    "from pyspark.sql.functions import regexp_replace, when, col, cast\n",
    "\n",
    "# display how the data contains\n",
    "display(fifa_df_v2.withColumn(\"Weight\",regexp_replace(\"Weight\",\"[0-9]\",\"\")).groupBy(\"Weight\").count().orderBy(\"Weight\"))\n",
    "\n",
    "\n",
    "fifa_df_v2=fifa_df_v2.withColumn(\"Weight\",\n",
    "                                 when(col(\"Weight\").like(\"%kg%\"),                   # finding the replacing the kg to \"\"\n",
    "                                      regexp_replace(\"Weight\",\"kg\",\"\").cast(\"int\")\n",
    "                                      )                                      \n",
    "                                 .when(col(\"Weight\").like(\"%lbs%\"),                  # finding the lbs and replacing the lbs to \"\" and converting the lbs to kg and casting to int\n",
    "                                       regexp_replace(\"Weight\",\"lbs\",\"\").cast(\"int\")*0.453592                                 \n",
    "                                       ).otherwise(None)\n",
    "                                 ).withColumn(\"Weight\",col(\"Weight\").cast(\"int\"))\n",
    "\n",
    "display(fifa_df_v2.select(\"Weight\"))\n",
    "\n",
    "display(fifa_df_v2.withColumn(\"Weight\",regexp_replace(\"Weight\",\"[0-9]\",\"\")).groupBy(\"Weight\").count().orderBy(\"Weight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60894d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REnaming the Columns to Hight to Height_in_cm and Weight to Weight_in_kg\n",
    "fifa_df_v2=fifa_df_v2.withColumnRenamed(\"Height\",\"Height_in_cm\").withColumnRenamed(\"Weight\",\"Weight_in_kg\")\n",
    "fifa_df_v2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting \"Value\",\"Wage\",\"Release Clause to int and \n",
    "# renameing into \"Values in Euro Million\", \"Wage in Euros\", \"Release Clause in Euro Million\"\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "import re\n",
    "def replacingValueWageRelaeaseClause(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    num=str(x)\n",
    "    num=re.sub(r\"[^0-9.]\",\"\",num)\n",
    "    if \"K\" in x:\n",
    "        return float(num)*1000\n",
    "    x=float(num)\n",
    "    \n",
    "    return x\n",
    "\n",
    "replaceingValueWageRelaeaseClause_=udf(replacingValueWageRelaeaseClause,StringType())\n",
    "\n",
    "fifa_df_v2=fifa_df_v2.withColumn(\"Value\",replaceingValueWageRelaeaseClause_(col(\"Value\"))).withColumn(\"Wage\",replaceingValueWageRelaeaseClause_(col(\"Wage\"))).withColumn(\"Release Clause\",replaceingValueWageRelaeaseClause_(col(\"Release Clause\"))).withColumnRenamed(\"Value\",\"Values in Euro Million\").withColumnRenamed(\"Wage\",\"Wage in Euros\").withColumnRenamed(\"Release Clause\",\"Release Clause in Euro Million\")\n",
    "fifa_df_v2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W/F, SM, IR  replacing the star an converting to int \n",
    "\n",
    "fifa_df_v2=fifa_df_v2.withColumn(\"W/F\",regexp_replace(col(\"W/F\"),\"â˜…\",\"\")).withColumn(\"SM\",regexp_replace(col(\"SM\"),\"â˜…\",\"\")).withColumn(\"IR\",regexp_replace(col(\"IR\"),\"â˜…\",\"\")).withColumn(\"W/F\",col(\"W/F\").cast(\"int\")).withColumn(\"SM\",col(\"SM\").cast(\"int\")).withColumn(\"IR\",col(\"IR\").cast(\"int\"))\n",
    "# Replacing the columns to int  \n",
    "fifa_df_v2=fifa_df_v2.withColumn(\"Crossing\",col(\"Crossing\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#display(fifa_df_v2.withColumn(\"Hits\",regexp_replace(\"Hits\",\"[0-9]\",\"\")).groupBy(\"Hits\").count().orderBy(\"Hits\"))\n",
    "\n",
    "\n",
    "def hitsData(x):\n",
    "    if x is None:\n",
    "        return 0\n",
    "    if \".K\" in x:\n",
    "        return float(x[:-2])*1000\n",
    "    if \"K\" in x:\n",
    "        return float(x[:-1])*1000\n",
    "    \n",
    "    return x   \n",
    "    \n",
    "\n",
    "hitsdatareplace=udf(hitsData,StringType())\n",
    "fifa_df_v2=fifa_df_v2.withColumn(\"Hits\",hitsdatareplace(col(\"Hits\")))#when(col(\"Hits\").like(\"%K%\"),regexp_replace(\"Hits\",\"[^0-9]\",\"\")*1000).otherwise(\"0\"))\n",
    "\n",
    "#display(fifa_df_v2.withColumn(\"Hits\",regexp_replace(\"Hits\",\"[0-9]\",\"\")).groupBy(\"Hits\").count().orderBy(\"Hits\"))\n",
    "\n",
    "display(fifa_df_v2.select(\"Hits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fifa_df_v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
